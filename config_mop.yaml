defaults:
  - agent: pbrl                                # td3_bc, cql, pbrl
  - override hydra/launcher: submitit_local

run_name: mop         # ndcg_lambda=${ndcg_lambda}_${data_type[0]}_${data_type[1]}

# unsupervised exploration
# expl_agent: td3
# task settings
env: walker
tasks: [walker_walk, walker_stand]                # main task to train (relable other datasets to this task)
data_type: [medium, medium]              # dataset for data sharing (corresponding each share_task)
share_data: False
data_sharing: False

teacher_dir: ../../teacher_models/${env}
cross_teacher: False
mode: a2a                                # critic2actor, actor2actor, critic2critic2actor
hidden_dim: 256
hidden_layers: 1

ndcg: False
ndcg_lambda: 0.1
ndcg_alpha: 0.5
sce: False

kendall: True
noise_vars: []

discount: 0.99
# train settings
num_grad_steps: 5
log_every_steps: 1
# eval
eval_every_steps: 1
num_eval_episodes: 1
# dataset
replay_buffer_dir: ../../collect    # make sure to update this if you change hydra run dir
replay_buffer_size: 100       # max: 10M
replay_buffer_num_workers: 4
batch_size: ${agent.batch_size}
# misc
seed: 1
device: cuda
save_video: True
use_tb: False

# used for train_offline_single
data_main: expert

deterministic_actor: True

wandb: True
hydra:
  run:
#    dir: ./result_td3bc_share/${task}-Share_${share_task[0]}_${share_task[1]}-Data_${data_type[0]}_${data_type[1]}-${agent.name}-${now:%m-%d-%H-%M-%S}
#    dir: ./result_cql/${task}-${data_main}-${agent.name}-${now:%m-%d-%H-%M-%S}
#    dir: ./result_pbrl/${task}-${data_main}-${agent.name}-${now:%m-%d-%H-%M-%S}
#    dir: ./result_pbrl_share/${task}-Share_${share_task[0]}_${share_task[1]}-${data_type[0]}-${agent.name}-${now:%m-%d-%H-%M-%S}
#    dir: ./output/${task}-${data_main}-${agent.name}-${now:%m-%d-%H-%M-%S}
    dir: ./result_mop/${run_name}-${now:%m-%d-%H-%M-%S}

